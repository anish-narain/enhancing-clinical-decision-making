# #1 Papers and Links

As someone who was reading research papers for the first time, this was a very useful guide on how to get started: [Fall 2022: Lecture 3 – “Shoulders of Giants”, by Pranav Rajpurkar](https://docs.google.com/document/d/1bPhwNdCCKkm1_adD0rx1YV6r2JG98qYmTxutT5gdAdQ/edit#heading=h.cpm24dharnhg). It is from Harvard's ["CS197 Harvard: AI Research Experiences" course](https://www.cs197.seas.harvard.edu/).

## Concept Bottleneck Models

| Papers and Links | Comments | Other |
| --------------- | --------------- | --------------- |
| Koh, Pang Wei, et al. "Concept bottleneck models." https://arxiv.org/pdf/2007.04612   | Where it all started (kind of). This is a very useful paper, which I often referred to throughout this project. | They also have a great [video presentation](https://slideslive.com/38928546/concept-bottleneck-models) for this paper and they made their [repo](https://github.com/yewsiang/ConceptBottleneck) public. |
| Margeloiu, Andrei, et al. "Do concept bottleneck models learn as intended?." https://arxiv.org/pdf/2105.04289    | Clearly outlines the different training methods of CBMs and some limitations.    | -    |
|  <ol><li>Havasi, Marton, **Sonali Parbhoo**, and Finale Doshi-Velez. "Addressing leakage in concept bottleneck models." [Link](https://finale.seas.harvard.edu/sites/scholar.harvard.edu/files/finale/files/10494_addressing_leakage_in_concept_.pdf).</li><li>Mahinpei, Anita, et al. "Promises and pitfalls of black-box concept learning models." https://arxiv.org/pdf/2106.13314</li></ol> | Useful papers for understanding leakage in CBMs.    | Havasi, **Parbhoo**, Doshi-Velez, Addressing leakage [repo](https://github.com/dtak/addressing-leakage)|
| <ol><li>Klimiene, Ugne, et al. "Multiview concept bottleneck models applied to diagnosing pediatric appendicitis." [Link](https://mds.inf.ethz.ch/fileadmin/user_upload/multiview_cbm_imlh_2022.pdf)</li><li>Yuksekgonul, Mert, Maggie Wang, and James Zou. "Post-hoc concept bottleneck models." https://arxiv.org/pdf/2205.15480</li></ol>| Some implementations of CBMs | <ol><li>Multiview CBM [repo](https://github.com/i6092467/semi-supervised-multiview-cbm) (really organised and nice to go through)</li><li>Post Hoc CBM [repo](https://github.com/mertyg/post-hoc-cbm)</li></ol>  |
| <ol><li>Oikarinen, Tuomas, et al. "Label-free concept bottleneck models." https://arxiv.org/pdf/2304.06129 </li><li>Ludan, Josh Magnus, et al. "Interpretable-by-Design Text Classification with Iteratively Generated Concept Bottleneck." [Link](https://paperswithcode.com/paper/interpretable-by-design-text-classification)</li><li>Kim, Injae, et al. "Concept bottleneck with visual concept filtering for explainable medical image classification." https://arxiv.org/pdf/2308.11920</li><li>Yan, An, et al. "Robust and interpretable medical image classifiers via concept bottleneck models." https://arxiv.org/pdf/2310.03182</li></ol>| Provides some examples of implementations of CBMs and how LLMs have been used with them (mostly for automated concept generation) | <ol><li>Label-free CBM [repo](https://github.com/Trustworthy-ML-Lab/Label-free-CBM/tree/main) </li><li>Textual Bottleneck Model [repo](https://github.com/JMRLudan/TBM/tree/main)</li></ol> |
| Wu, Carissa, et al. "Learning optimal summaries of clinical time-series with concept bottleneck models." [Link](https://proceedings.mlr.press/v182/wu22a/wu22a.pdf) | Another one of **Dr Parbhoo's** papers. It gives a good insight into using a CBM on MIMIC III | Timeseries CBM for MIMIC III [repo](https://github.com/dtak/optimal-summaries-public) |

## Large Language Models

| Resources | Comments |
| --------------- | --------------- |
| <ol><li>[Cloudflare](https://www.cloudflare.com/en-gb/learning/ai/what-is-large-language-model/)</li><li>[Elastic](https://www.elastic.co/what-is/large-language-models) </li><li>Naveed, Humza, et al. "A comprehensive overview of large language models." https://arxiv.org/pdf/2307.06435</li></ol>    | There are plenty of resources online for learning about LLMs, these are some that I referenced in my [Interim Report](https://github.com/anish-narain/final-year-project/blob/main/resources/interim-report.pdf).    |
| Zhou, Hongjian, et al. "A survey of large language models in medicine: Progress, application, and challenge." https://arxiv.org/pdf/2311.05112   | Really great overview of the current state of the art of LLMs in medicine (at the time of writing).  It comes with an amazing GitHub [repo](https://github.com/AI-in-Health/MedLLMsPracticalGuide) that has a "curated list of practical guide resources of Medical LLMs (Medical LLMs Tree, Tables, and Papers)".  |
| <ol><li>Agrawal, Monica, et al. "Large language models are few-shot clinical information extractors." https://arxiv.org/pdf/2205.12689</li><li>Goel, Akshay, et al. "Llms accelerate annotation for medical information extraction." https://arxiv.org/pdf/2312.02296</li></ol>    | LLMs and Medical Information Extraction   |

## Relevant Medical/MIMIC Based Projects

| Resources | Comments |
| --------------- | --------------- |
| Ghassemi, Marzyeh, et al. "Predicting intervention onset in the ICU with switching state space models." [Link](https://pubmed.ncbi.nlm.nih.gov/28815112/)   | Identifying patterns in ICU patients’ vital signs and predicting one of five treatments for the patient, using MIMIC III. |
| E. Lehman and A. Johnson, Clinical-T5: Large Language Models Built Using MIMIC Clinical Text [(version 1.0.0)](https://physionet.org/content/clinical-t5/1.0.0/)   | LLMs built using MIMIC III and IV  |
| <ol><li>Nowroozilarki, Zhale, et al. "Real-time mortality prediction using MIMIC-IV ICU data via boosted nonparametric hazards." https://arxiv.org/pdf/2110.08949</li><li>Huang, Tianzhi, et al. "Machine learning for prediction of in-hospital mortality in lung cancer patients admitted to intensive care unit." [Link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9879439/)</li><li>Kumar, Sayantan, et al. "Self-explaining neural network with concept-based explanations for ICU mortality prediction." [Link](https://dl.acm.org/doi/pdf/10.1145/3535508.3545547)</li><li>Pang, Ke, et al. "Establishment of ICU mortality risk prediction models with machine learning algorithm using MIMIC-IV database." [Link](https://pubmed.ncbi.nlm.nih.gov/35626224/)</li></ol> | Predicting mortality using MIMIC IV |
| Medication extraction labels for mimic-iv-note clinical database [(version 1.0.0)](https://physionet.org/content/medication-labels-mimic-note/1.0.0/) | Text Annotations from Goel, Akshay, et al. "Llms accelerate annotation for medical information extraction." on Physionet. |
| <ol><li>Wayne, Max T., et al. "Electronic “Sniffer” systems to identify the acute respiratory distress syndrome." [Link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6441701/#:~:text=Individual%20ARDS%20sniffer%20systems%20can,with%20ARDS%20in%20real%20time.)</li><li>McKown, Andrew C., et al. "External validity of electronic sniffers for automated recognition of acute respiratory distress syndrome." [Link](https://journals.sagepub.com/doi/abs/10.1177/0885066617720159)</li><li>Reamaroon, Narathip, et al. "Accounting for label uncertainty in machine learning for detection of acute respiratory distress syndrome. [Link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6351314/)</li></ol> | ML for Identifying ARDS |
| <ol><li>Mayampurath, Anoop, et al. "External validation of an acute respiratory distress syndrome prediction model using radiology reports." [Link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7872467/)</li><li>Afshar, Majid, et al. "A computable phenotype for acute respiratory distress syndrome using natural language processing and machine learning." [Link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6371271/)</li><li>Afshin-Pour, Babak, et al. "Discriminating Acute Respiratory Distress Syndrome from other forms of respiratory failure via iterative machine learning." [Link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9812471/)</li><li>Gandomi, Amir, et al. "ARDSFlag: An NLP/Machine Learning Algorithm to Visualize and Detect High-Probability ARDS Admissions Independent of Provider Recognition and Billing Codes." [Link](https://www.medrxiv.org/content/10.1101/2022.09.27.22280416v2.full)</li><li>Pathak, Ashwin, et al. "RespBERT: A multi-site validation of a Natural Language Processing algorithm, of Radiology Notes to Identify Acute Respiratory Distress Syndrome (ARDS)." [Link](https://www.researchgate.net/publication/370387042_RespBERT_A_multi-site_validation_of_a_Natural_Language_Processing_algorithm_of_Radiology_Notes_to_Identify_Acute_Respiratory_Distress_Syndrome_ARDS)</li></ol> | Identifying ARDS with NLP |


# #2 MIMIC Resources
1. [MIMIC Docs](https://mimic.mit.edu/docs/): useful for getting an overview of what's in the different MIMIC files.
2. [Setting up Google BigQuery for MIMIC](https://mimic.mit.edu/docs/gettingstarted/cloud/bigquery/): this is a very useful, storage efficient way of querying the MIMIC data. You can write the query scripts in BigQuery and download the necessary CSV files. There are added features like creating a Python notebook for your data so you can give more visual information about it. Furthermore, it provides additional tables like mimiciv_derived which makes querying easier and provides useful information about the tables that you can easily flick through while you are writing your queries. I would highly recommend using this tool.
3. [MIMIC Scripts for Google BigQuery](https://github.com/MIT-LCP/mimic-code/tree/main/mimic-iv): another big reason for using Google BigQuery. You have lots of scripts ready to use. You can query things like `mimic_derivedtable` which is very useful.






